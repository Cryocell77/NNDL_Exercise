# RNN诗歌生成实验报告


## 1. 模型原理解释

### 1.1 RNN (Recurrent Neural Network)

循环神经网络是一种专门处理序列数据的神经网络架构：

- **核心特点**：具有"记忆"能力，能够处理变长序列
- **结构特征**：隐藏状态在时间步之间传递，形成循环连接
- **数学表示**：h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b_h)
- **应用场景**：语言建模、机器翻译、时序预测等

**优势**：

- 能够处理任意长度的序列
- 参数共享，模型相对简单
- 理论上可以捕获长期依赖

**劣势**：

- 梯度消失问题，难以学习长期依赖
- 训练效率相对较低

### 1.2 LSTM (Long Short-Term Memory)

长短期记忆网络是RNN的改进版本：

- **核心创新**：引入门控机制(遗忘门、输入门、输出门)
- **记忆单元**：通过细胞状态C_t维护长期记忆
- **门控公式**：
  - 遗忘门：f_t = σ(W_f * [h_{t-1}, x_t] + b_f)
  - 输入门：i_t = σ(W_i * [h_{t-1}, x_t] + b_i)
  - 输出门：o_t = σ(W_o * [h_{t-1}, x_t] + b_o)

**优势**：

- 有效解决梯度消失问题
- 能够学习长期依赖关系
- 在实际应用中表现优异

### 1.3 GRU (Gated Recurrent Unit)

门控循环单元是LSTM的简化版本：

- **结构特点**：只有两个门(重置门和更新门)
- **参数更少**：相比LSTM计算效率更高
- **门控机制**：
  - 重置门：r_t = σ(W_r * [h_{t-1}, x_t])
  - 更新门：z_t = σ(W_z * [h_{t-1}, x_t])

**优势**：

- 参数少，训练速度快
- 性能接近LSTM
- 结构简单，易于实现

## 2. 诗歌生成过程分析

本节详细分析RNN诗歌生成的完整流程，从数据预处理到最终诗歌输出的每个关键步骤。

### 2.1 数据预处理阶段

**步骤1：文本数据读取与清洗**
- 从`poems.txt`文件中逐行读取古诗数据
- 使用`':'`分隔符提取诗歌内容，去除标题信息
- 过滤长度超过200字符的诗歌，确保训练效率

**步骤2：序列标记化处理**
- 为每首诗添加开始标记`'bos'`(beginning of sequence)
- 在诗歌末尾添加结束标记`'eos'`(end of sequence)
- 将诗歌转换为字符级序列：`['bos', '春', '眠', '不', '觉', '晓', 'eos']`

**步骤3：词汇表构建**
- 统计所有字符的出现频率
- 按频率降序排列，构建字符到ID的映射词典
- 添加特殊标记：`'PAD'`(填充)、`'UNK'`(未知字符)

**步骤4：数据编码与批处理**
- 将字符序列转换为数字ID序列
- 使用`tf.data.Dataset`进行批量处理和填充
- 将序列分为输入序列(x)和目标序列(y)，错位一个时间步

### 2.2 诗歌生成的核心步骤

**第一步：模型初始化**
```python
# 初始化RNN隐藏状态
state = model.rnncell.get_initial_state(batch_size=1, dtype=tf.float32)
# 设置起始词汇作为第一个输入
cur_token = tf.constant([word2id[begin_word]], dtype=tf.int32)
```

**第二步：逐字符生成循环**
```python
for step in range(max_length):
    # 1. 词嵌入：将当前字符ID转换为向量
    inp_emb = model.embed_layer(cur_token)  # shape: [1, 64]
    
    # 2. RNN前向传播：计算新的隐藏状态
    h, state = model.rnncell.call(inp_emb, state)  # shape: [1, 128]
    
    # 3. 输出层：预测下一个字符的概率分布
    logits = model.dense(h)  # shape: [1, vocab_size]
    
    # 4. 解码：选择概率最大的字符作为输出
    cur_token = tf.argmax(logits, axis=-1)  # shape: [1]
    
    # 5. 记录生成的字符
    generated_chars.append(id2word[cur_token.numpy()[0]])
```

**第三步：格式化输出**
- 过滤非汉字字符，确保输出质量
- 按照五言绝句格式组织：每行5字，共4行
- 添加适当的标点符号：奇数行加逗号，偶数行加句号

### 2.3 生成过程的技术细节

**字符级语言建模**
- 模型基于字符级别进行预测，每次生成一个汉字
- 通过前面的字符序列预测下一个字符的概率分布
- 使用贪心解码策略：选择概率最高的字符

**状态传递机制**
- RNN的隐藏状态在生成过程中持续传递
- 隐藏状态包含了前面所有字符的历史信息
- 这种"记忆"机制使模型能够生成连贯的诗句

**生成控制策略**
- **长度控制**：限制生成诗歌的总字符数(通常20字)
- **格式控制**：按照5×4的格式要求组织输出
- **质量过滤**：去除生僻字和无意义字符

### 2.4 完整生成流程示例

以生成"日"字开头的诗歌为例：

```
输入: "日" → 嵌入层 → RNN状态更新
↓
预测: "云" → 更新状态 → 继续预测
↓  
逐步生成: "日云声向东风不可得一枝犹是一枝声来不得无"
↓
格式化输出:
日云声向东，
风不可得一。
枝犹是一枝，
声来不得无。
```


## 3. 实验结果展示

### 3.1 训练过程

模型训练10个epoch，损失从8.82逐步下降到5.28，表明模型成功学习了诗歌的语言模式。

#### 训练截图：

![训练过程](.\img\train.png)

### 3.2 生成诗歌结果

#### 以"日"开头的诗歌：

```
日云声向东，
风不可得一。
枝犹是一枝，
声来不得无。
```

#### 以"红"开头的诗歌：

```
红递浯蓉阳，
厂蓉纟蓉滦。
蓉疠蓉滦蓉，
壒蓉屿滦蓉。
```

#### 以"山"开头的诗歌：

```
山上山风客，
不知何处处。
一枝犹是一，
枝声来不得。
```

#### 以"夜"开头的诗歌：

```
夜风吹落水，
深来无处处。
不得不知君，
客无人事何。
```

#### 以"湖"开头的诗歌：

```
湖水中春色，
风风不见春。
风月不知何，
处处一枝犹。
```

#### 以"海"开头的诗歌：

```
海山上风吹，
落月深风吹。
落月风雨满，
云风客无人。
```

#### 以"月"开头的诗歌：

```
月花满风吹，
一月深云不。
可见山上不，
相逢客无人。
```

#### 结果截图：

![实验结果](.\img\poem.png)

### 3.3 结果分析

1. **格式符合**：生成的诗歌基本符合五言绝句的格式要求
2. **语义连贯**：部分诗句具有一定的语义连贯性和意境
3. **存在问题**：
   - 部分生成结果包含生僻字或无意义字符
   - 语法和语义的连贯性有待提升
   - 韵律和平仄规律学习不够充分

## 4. 实验总结

### 4.1 成果

1. 成功实现了基于RNN的中文诗歌生成系统
2. 模型能够学习古诗的基本语言模式
3. 生成的诗歌在格式上基本符合五言绝句要求
4. 实现了指定开头字符的可控生成

### 4.2 不足与改进方向

1. **数据质量**：需要更高质量的训练数据和更好的数据预处理
2. **模型架构**：可以尝试使用LSTM或GRU替代简单RNN
3. **生成策略**：可以采用束搜索(beam search)等更好的解码策略
4. **约束机制**：加入韵律、平仄等诗歌特有的约束条件
5. **评估指标**：建立更完善的诗歌质量评估体系

### 4.3 技术收获

1. 深入理解了RNN在序列生成任务中的应用
2. 掌握了TensorFlow 2.0的模型构建和训练方法
3. 学习了自然语言处理中的数据预处理技巧
4. 了解了文本生成任务的评估和优化方法
