{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 诗歌生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "\n",
    "start_token = 'bos'\n",
    "end_token = 'eos'\n",
    "\n",
    "def process_dataset(fileName):\n",
    "    examples = []\n",
    "    with open(fileName, 'r', encoding='utf-8') as fd:\n",
    "        for line in fd:\n",
    "            outs = line.strip().split(':')\n",
    "            content = ''.join(outs[1:])\n",
    "            ins = [start_token] + list(content) + [end_token] \n",
    "            if len(ins) > 200:\n",
    "                continue\n",
    "            examples.append(ins)\n",
    "            \n",
    "    counter = collections.Counter()\n",
    "    for e in examples:\n",
    "        for w in e:\n",
    "            counter[w]+=1\n",
    "    \n",
    "    sorted_counter = sorted(counter.items(), key=lambda x: -x[1])  # 排序\n",
    "    words, _ = zip(*sorted_counter)\n",
    "    words = ('PAD', 'UNK') + words[:len(words)]\n",
    "    word2id = dict(zip(words, range(len(words))))\n",
    "    id2word = {word2id[k]:k for k in word2id}\n",
    "    \n",
    "    indexed_examples = [[word2id[w] for w in poem]\n",
    "                        for poem in examples]\n",
    "    seqlen = [len(e) for e in indexed_examples]\n",
    "    \n",
    "    instances = list(zip(indexed_examples, seqlen))\n",
    "    \n",
    "    return instances, word2id, id2word\n",
    "\n",
    "def poem_dataset():\n",
    "    instances, word2id, id2word = process_dataset('./poems.txt')\n",
    "    ds = tf.data.Dataset.from_generator(lambda: [ins for ins in instances], \n",
    "                                            (tf.int64, tf.int64), \n",
    "                                            (tf.TensorShape([None]),tf.TensorShape([])))\n",
    "    ds = ds.shuffle(buffer_size=10240)\n",
    "    ds = ds.padded_batch(100, padded_shapes=(tf.TensorShape([None]),tf.TensorShape([])))\n",
    "    ds = ds.map(lambda x, seqlen: (x[:, :-1], x[:, 1:], seqlen-1))\n",
    "    return ds, word2id, id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型代码， 完成建模代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myRNNModel(keras.Model):\n",
    "    def __init__(self, w2id):\n",
    "        super(myRNNModel, self).__init__()\n",
    "        self.v_sz = len(w2id)\n",
    "        self.embed_layer = tf.keras.layers.Embedding(self.v_sz, 64, \n",
    "                                                    batch_input_shape=[None, None])\n",
    "        \n",
    "        self.rnncell = tf.keras.layers.SimpleRNNCell(128)\n",
    "        self.rnn_layer = tf.keras.layers.RNN(self.rnncell, return_sequences=True)\n",
    "        self.dense = tf.keras.layers.Dense(self.v_sz)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inp_ids):\n",
    "        '''\n",
    "        此处完成建模过程，可以参考Learn2Carry\n",
    "        '''\n",
    "        x = self.embed_layer(inp_ids)\n",
    "        rnn_out = self.rnn_layer(x)\n",
    "        logits = self.dense(rnn_out)\n",
    "        return logits\n",
    "    \n",
    "    @tf.function\n",
    "    def get_next_token(self, x, state):\n",
    "        '''\n",
    "        shape(x) = [b_sz,] \n",
    "        '''\n",
    "    \n",
    "        inp_emb = self.embed_layer(x) #shape(b_sz, emb_sz)\n",
    "        h, state = self.rnncell.call(inp_emb, state) # shape(b_sz, h_sz)\n",
    "        logits = self.dense(h) # shape(b_sz, v_sz)\n",
    "        out = tf.argmax(logits, axis=-1)\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个计算sequence loss的辅助函数，只需了解用途。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkMask(input_tensor, maxLen):\n",
    "    shape_of_input = tf.shape(input_tensor)\n",
    "    shape_of_output = tf.concat(axis=0, values=[shape_of_input, [maxLen]])\n",
    "\n",
    "    oneDtensor = tf.reshape(input_tensor, shape=(-1,))\n",
    "    flat_mask = tf.sequence_mask(oneDtensor, maxlen=maxLen)\n",
    "    return tf.reshape(flat_mask, shape_of_output)\n",
    "\n",
    "\n",
    "def reduce_avg(reduce_target, lengths, dim):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        reduce_target : shape(d_0, d_1,..,d_dim, .., d_k)\n",
    "        lengths : shape(d0, .., d_(dim-1))\n",
    "        dim : which dimension to average, should be a python number\n",
    "    \"\"\"\n",
    "    shape_of_lengths = lengths.get_shape()\n",
    "    shape_of_target = reduce_target.get_shape()\n",
    "    if len(shape_of_lengths) != dim:\n",
    "        raise ValueError(('Second input tensor should be rank %d, ' +\n",
    "                         'while it got rank %d') % (dim, len(shape_of_lengths)))\n",
    "    if len(shape_of_target) < dim+1 :\n",
    "        raise ValueError(('First input tensor should be at least rank %d, ' +\n",
    "                         'while it got rank %d') % (dim+1, len(shape_of_target)))\n",
    "\n",
    "    rank_diff = len(shape_of_target) - len(shape_of_lengths) - 1\n",
    "    mxlen = tf.shape(reduce_target)[dim]\n",
    "    mask = mkMask(lengths, mxlen)\n",
    "    if rank_diff!=0:\n",
    "        len_shape = tf.concat(axis=0, values=[tf.shape(lengths), [1]*rank_diff])\n",
    "        mask_shape = tf.concat(axis=0, values=[tf.shape(mask), [1]*rank_diff])\n",
    "    else:\n",
    "        len_shape = tf.shape(lengths)\n",
    "        mask_shape = tf.shape(mask)\n",
    "    lengths_reshape = tf.reshape(lengths, shape=len_shape)\n",
    "    mask = tf.reshape(mask, shape=mask_shape)\n",
    "\n",
    "    mask_target = reduce_target * tf.cast(mask, dtype=reduce_target.dtype)\n",
    "\n",
    "    red_sum = tf.reduce_sum(mask_target, axis=[dim], keepdims=False)\n",
    "    red_avg = red_sum / (tf.cast(lengths_reshape, dtype=tf.float32) + 1e-30)\n",
    "    return red_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义loss函数，定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(logits, labels, seqlen):\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels)\n",
    "    losses = reduce_avg(losses, seqlen, dim=1)\n",
    "    return tf.reduce_mean(losses)\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(model, optimizer, x, y, seqlen):\n",
    "    '''\n",
    "    完成一步优化过程，可以参考之前做过的模型\n",
    "    '''\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = compute_loss(logits, y, seqlen)\n",
    "\n",
    "    # compute gradient\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def train(epoch, model, optimizer, ds):\n",
    "    loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    for step, (x, y, seqlen) in enumerate(ds):\n",
    "        loss = train_one_step(model, optimizer, x, y, seqlen)\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            print('epoch', epoch, ': loss', loss.numpy())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练优化过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : loss 8.82041\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function train_one_step at 0x0000023298AFE4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 14 calls to <function train_one_step at 0x0000023298AFE4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "epoch 1 : loss 6.530802\n",
      "epoch 2 : loss 6.179639\n",
      "epoch 3 : loss 5.9114075\n",
      "epoch 4 : loss 5.7746215\n",
      "epoch 5 : loss 5.475292\n",
      "epoch 6 : loss 5.539743\n",
      "epoch 7 : loss 5.381737\n",
      "epoch 8 : loss 5.326028\n",
      "epoch 9 : loss 5.2763305\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Adam(0.0005)\n",
    "train_ds, word2id, id2word = poem_dataset()\n",
    "model = myRNNModel(word2id)\n",
    "\n",
    "for epoch in range(10):\n",
    "    loss = train(epoch, model, optimizer, train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此人不得得，不知何处是何人。eos来不得无人事，不得人间不可知。eos道不知何处处，一枝犹是一枝声。eos来不得\n"
     ]
    }
   ],
   "source": [
    "def gen_sentence():\n",
    "    state = [tf.random.normal(shape=(1, 128), stddev=0.5), tf.random.normal(shape=(1, 128), stddev=0.5)]\n",
    "    cur_token = tf.constant([word2id['bos']], dtype=tf.int32)\n",
    "    collect = []\n",
    "    for _ in range(50):\n",
    "        cur_token, state = model.get_next_token(cur_token, state)\n",
    "        collect.append(cur_token.numpy()[0])\n",
    "    return [id2word[t] for t in collect]\n",
    "print(''.join(gen_sentence()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以'日'开头的诗歌：\n",
      "日何处在，\n",
      "风雨落花。\n",
      "\n",
      "\n",
      "\n",
      "以'红'开头的诗歌：\n",
      "红嵘。_蓉\n",
      "彧蓉釦蓉昈\n",
      "bos箘蓉濆蓉\n",
      "昈蓉昈蓉滦\n",
      "\n",
      "以'山'开头的诗歌：\n",
      "山畔滨，恐\n",
      "蓉滨蓉疠蓉\n",
      "洲赗蓉滦蓉\n",
      "屿屿滦蓉壒\n",
      "\n",
      "以'夜'开头的诗歌：\n",
      "夜夕。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "以'湖'开头的诗歌：\n",
      "湖日暮水中\n",
      "。\n",
      "\n",
      "\n",
      "\n",
      "以'海'开头的诗歌：\n",
      "海上人。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "以'月'开头的诗歌：\n",
      "月中，不得\n",
      "无人。\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_poem_5x4(begin_word, model, word2id, id2word):\n",
    "    \"\"\"\n",
    "    生成5言绝句（5字×4行），并以指定词汇开头\n",
    "    \n",
    "    参数:\n",
    "        begin_word (str): 开头词汇（如\"日\"、\"红\"等）\n",
    "        model: 诗歌生成模型（需实现get_next_token方法）\n",
    "        word2id (dict): 词汇到ID的映射\n",
    "        id2word (dict): ID到词汇的映射\n",
    "    \n",
    "    返回:\n",
    "        str: 格式化的5×4诗歌（每行5字，共4行）\n",
    "    \"\"\"\n",
    "    if begin_word not in word2id:\n",
    "        raise ValueError(f\"开头词 '{begin_word}' 不在词典中\")\n",
    "\n",
    "    # 初始化状态和起始token\n",
    "    state = [tf.random.normal(shape=(1, 128)), tf.random.normal(shape=(1, 128))]\n",
    "    cur_token = tf.constant([word2id[begin_word]], dtype=tf.int32)\n",
    "    poem_ids = [word2id[begin_word]]  # 记录所有生成的token（包含开头词）\n",
    "\n",
    "    # 生成20个字（5字×4行），遇到结束符提前终止\n",
    "    while len(poem_ids) < 20:\n",
    "        cur_token, state = model.get_next_token(cur_token, state)\n",
    "        token_id = cur_token.numpy()[0]\n",
    "        if id2word[token_id] in [\"eos\", \"<END>\"]:  # 结束符检查\n",
    "            break\n",
    "        poem_ids.append(token_id)\n",
    "\n",
    "    # 转换为文字并格式化为5×4\n",
    "    poem_chars = [id2word[t] for t in poem_ids[:20]]  # 确保最多20字\n",
    "    poem_lines = [\n",
    "        ''.join(poem_chars[i*5 : (i+1)*5])  # 每行5字\n",
    "        for i in range(4)\n",
    "    ]\n",
    "    return '\\n'.join(poem_lines)  # 用换行符连接4行\n",
    "\n",
    "# 示例用法\n",
    "begin_words = [\"日\", \"红\", \"山\", \"夜\", \"湖\", \"海\", \"月\"]\n",
    "for word in begin_words:\n",
    "    print(f\"以'{word}'开头的诗歌：\")\n",
    "    poem = generate_poem_5x4(word, model, word2id, id2word)\n",
    "    print(poem)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以'日'开头的诗歌：\n",
      "日云声向东，\n",
      "风不可得一。\n",
      "枝犹是一枝，\n",
      "声来不得无。\n",
      "\n",
      "以'红'开头的诗歌：\n",
      "红递浯蓉阳，\n",
      "厂蓉纟蓉滦。\n",
      "蓉疠蓉滦蓉，\n",
      "壒蓉屿滦蓉。\n",
      "\n",
      "以'山'开头的诗歌：\n",
      "山上山风客，\n",
      "不知何处处。\n",
      "一枝犹是一，\n",
      "枝声来不得。\n",
      "\n",
      "以'夜'开头的诗歌：\n",
      "夜风吹落水，\n",
      "深来无处处。\n",
      "不得不知君，\n",
      "客无人事何。\n",
      "\n",
      "以'湖'开头的诗歌：\n",
      "湖水中春色，\n",
      "风风不见春。\n",
      "风月不知何，\n",
      "处处一枝犹。\n",
      "\n",
      "以'海'开头的诗歌：\n",
      "海山上风吹，\n",
      "落月深风吹。\n",
      "落月风雨满，\n",
      "云风客无人。\n",
      "\n",
      "以'月'开头的诗歌：\n",
      "月花满风吹，\n",
      "一月深云不。\n",
      "可见山上不，\n",
      "相逢客无人。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_chinese_char(char):\n",
    "    \"\"\"检查字符是否为汉字\"\"\"\n",
    "    return '\\u4e00' <= char <= '\\u9fff'\n",
    "\n",
    "def format_poem(poem_lines):\n",
    "    \"\"\"修正标点，使其仅出现在行末\"\"\"\n",
    "    formatted_lines = []\n",
    "    \n",
    "    for i, line in enumerate(poem_lines):\n",
    "        # 移除所有非汉字字符（确保之前已经处理过）\n",
    "        line = ''.join(filter(is_chinese_char, line))\n",
    "        \n",
    "        # 重新调整标点：保证句尾有适当标点\n",
    "        if i % 2 == 0:  # 1、3句为逗号\n",
    "            line += '，'\n",
    "        else:  # 2、4句为句号\n",
    "            line += '。'\n",
    "\n",
    "        formatted_lines.append(line)\n",
    "    \n",
    "    return formatted_lines\n",
    "\n",
    "def gen_poem(begin_word, max_lines=4, line_length=5):\n",
    "    \"\"\"生成以指定汉字开头的完整唐诗，并保证换行格式\"\"\"\n",
    "\n",
    "    # 检查起始字是否为汉字\n",
    "    if not is_chinese_char(begin_word):\n",
    "        begin_word = '春'  # 默认值\n",
    "    \n",
    "    cur_token = tf.constant([word2id.get(begin_word, word2id['bos'])], dtype=tf.int32)\n",
    "    state = model.rnncell.get_initial_state(batch_size=1, dtype=tf.float32)\n",
    "\n",
    "    poem = []\n",
    "    line_tokens = []\n",
    "\n",
    "    while len(poem) < max_lines:\n",
    "        cur_token, state = model.get_next_token(cur_token, state)\n",
    "        token_id = cur_token.numpy()[0]\n",
    "\n",
    "        if token_id == word2id['eos']:  # 跳过无意义终止符\n",
    "            continue\n",
    "            \n",
    "        char = id2word.get(token_id, '')\n",
    "        \n",
    "        # 只保留汉字字符\n",
    "        if is_chinese_char(char):\n",
    "            line_tokens.append(char)\n",
    "\n",
    "        # 如果一行达到了指定长度且不超过最大行数，添加到诗歌中\n",
    "        if len(line_tokens) == line_length:\n",
    "            poem.append(''.join(line_tokens))\n",
    "            line_tokens = []\n",
    "\n",
    "        # 如果达到了最大行数，结束\n",
    "        if len(poem) == max_lines:\n",
    "            break\n",
    "\n",
    "    # 确保以指定的字开头\n",
    "    if poem:\n",
    "        poem[0] = begin_word + poem[0][1:] if len(poem[0]) > 1 else begin_word\n",
    "\n",
    "    return '\\n'.join(format_poem(poem))  # 修正标点\n",
    "\n",
    "# 测试生成\n",
    "begin_words = [\"日\", \"红\", \"山\", \"夜\", \"湖\", \"海\", \"月\"]\n",
    "\n",
    "for word in begin_words:\n",
    "    print(f\"以'{word}'开头的诗歌：\")\n",
    "    print(gen_poem(word, max_lines=4))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
